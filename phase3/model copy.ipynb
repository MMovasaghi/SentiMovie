{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/m213/Library/anaconda3/envs/mlsd/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi import UploadFile, File, Form, HTTPException\n",
    "import transformers\n",
    "from typing import Annotated\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "import mlflow\n",
    "import time\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=['*'],\n",
    "    allow_credentials=['*'],\n",
    "    allow_methods=['*'],\n",
    "    allow_headers=['*'],\n",
    ")\n",
    "\n",
    "def logging_run(text):\n",
    "    log = f\"INFO:     {text}\"\n",
    "    print(log)\n",
    "\n",
    "class InputJson(BaseModel):\n",
    "    texts: list\n",
    "\n",
    "round_number = 4\n",
    "\n",
    "classifier = transformers.pipeline(model='Movasaghi/finetuning-sentiment-rottentomatoes', \n",
    "                                   return_all_scores=True)\n",
    "# classifier.eval()\n",
    "\n",
    "def monitor_hardware():\n",
    "    cpu_percent = psutil.cpu_percent() \n",
    "    memory_percent = psutil.virtual_memory().percent\n",
    "    return cpu_percent, memory_percent\n",
    "\n",
    "\n",
    "def logging_mlflow(start_time):\n",
    "    latency = time.time() - start_time\n",
    "    logging_run(f\"Latency: {latency:.5f}s\")\n",
    "    cpu_percent, memory_percent = monitor_hardware()\n",
    "    mlflow.log_metric(\"Latency\", latency)\n",
    "    mlflow.log_metric(\"CPU Usage\", cpu_percent) \n",
    "    mlflow.log_metric(\"Memory Usage\", memory_percent) \n",
    "\n",
    "\n",
    "async def batch_prediction(texts):\n",
    "    logging_run(f\"Batch Inference (Number: {len(texts)})\")\n",
    "    output= classifier(texts)\n",
    "    results = {\n",
    "        \"sentiment\": None,\n",
    "        \"score\": None,\n",
    "        \"detail\": []\n",
    "        }\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    for i in range(len(output)):\n",
    "        result = {\n",
    "            \"text\": texts[i], \n",
    "            \"sentiment\": None,\n",
    "            \"probability\": None\n",
    "            }\n",
    "        if output[i][0]['score'] > output[i][1]['score']:\n",
    "            result['sentiment'] = \"Negative\"\n",
    "            result['probability'] = round(output[i][0]['score'], round_number)\n",
    "            negative_count += 1\n",
    "        else:\n",
    "            result['sentiment'] = \"Positive\"\n",
    "            result['probability'] = round(output[i][1]['score'], round_number)\n",
    "            positive_count += 1\n",
    "        results[\"detail\"].append(result)\n",
    "    results['sentiment'] = \"Positive\" if positive_count > negative_count else \"Negative\"\n",
    "    results['score'] = round(positive_count / (negative_count + positive_count), round_number)\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "async def single_prediction(text):\n",
    "    logging_run(f\"Single Inference\")\n",
    "    output= classifier(text)\n",
    "    result = {\n",
    "        \"text\": text, \n",
    "        \"sentiment\": None,\n",
    "        \"probability\": None\n",
    "        }\n",
    "    if output[0][0]['score'] > output[0][1]['score']:\n",
    "        result['sentiment'] = \"Negative\"\n",
    "        result['probability'] = round(output[0][0]['score'], round_number)\n",
    "    else:\n",
    "        result['sentiment'] = \"Positive\"\n",
    "        result['probability'] = round(output[0][1]['score'], round_number)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(data: InputJson):\n",
    "    start_time = time.time()\n",
    "\n",
    "    result = None\n",
    "    if len(data.texts) > 1:\n",
    "        result = await batch_prediction(data.texts)\n",
    "    elif len(data.texts) == 1:\n",
    "        result = await single_prediction(data.texts[0])\n",
    "    else:\n",
    "        raise HTTPException(status_code=400, detail=\"You must send at least one text.\")\n",
    "\n",
    "    logging_mlflow(start_time)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [5105]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     Batch Inference (Number: 6)\n",
      "INFO:     Latency: 0.13576s\n",
      "INFO:     127.0.0.1:55000 - \"POST /predict HTTP/1.1\" 200 OK\n",
      "INFO:     Batch Inference (Number: 6)\n",
      "INFO:     Latency: 0.20246s\n",
      "INFO:     127.0.0.1:55045 - \"POST /predict HTTP/1.1\" 200 OK\n",
      "INFO:     Batch Inference (Number: 6)\n",
      "INFO:     Latency: 0.19129s\n",
      "INFO:     127.0.0.1:55242 - \"POST /predict HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [5105]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-conda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
