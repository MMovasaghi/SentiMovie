{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi import UploadFile, File, Form, HTTPException\n",
    "import transformers\n",
    "from typing import Annotated\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "import mlflow\n",
    "import asyncio\n",
    "import time\n",
    "import psutil\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=['*'],\n",
    "    allow_credentials=['*'],\n",
    "    allow_methods=['*'],\n",
    "    allow_headers=['*'],\n",
    ")\n",
    "\n",
    "def logging_run(text):\n",
    "    log = f\"INFO:     {text}\"\n",
    "    print(log)\n",
    "\n",
    "class InputJson(BaseModel):\n",
    "    texts: list\n",
    "\n",
    "\n",
    "# -- must set with database\n",
    "model_A_url = 'http://0.0.0.0:8000/predict'\n",
    "model_B_url = 'http://0.0.0.0:8001/predict'\n",
    "output_other = []\n",
    "\n",
    "\n",
    "def monitor_hardware():\n",
    "    cpu_percent = psutil.cpu_percent() \n",
    "    memory_percent = psutil.virtual_memory().percent\n",
    "    return cpu_percent, memory_percent\n",
    "\n",
    "\n",
    "def logging_mlflow(start_time, item=0):\n",
    "    latency = time.time() - start_time\n",
    "    logging_run(f\"Latency_{item}: {latency:.5f}s\")\n",
    "    cpu_percent, memory_percent = monitor_hardware()\n",
    "    mlflow.log_metric(f\"Latency_{item}\", latency)\n",
    "    mlflow.log_metric(f\"CPU_Usage_{item}\", cpu_percent) \n",
    "    mlflow.log_metric(f\"Memory_Usage_{item}\", memory_percent) \n",
    "\n",
    "\n",
    "async def model_inference(data, model_url, model_main=True):\n",
    "    try:\n",
    "        response = requests.post(model_url, json=data)\n",
    "        if model_main:\n",
    "            if response.status_code != 200:\n",
    "                raise HTTPException(status_code=response.status_code)\n",
    "        return response.json()\n",
    "    except:\n",
    "        raise HTTPException(status_code=500, detail=\"The model not working\")\n",
    "    \n",
    "\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(data: InputJson):\n",
    "    start_time0 = time.time()\n",
    "    input_data = {\"texts\": data.texts}\n",
    "    # -- base server inference\n",
    "    start_time = time.time()\n",
    "    response = await model_inference(input_data, model_A_url, True)\n",
    "    logging_mlflow(start_time, \"Base\")\n",
    "    \n",
    "    # -- other servers inference\n",
    "    start_time = time.time()\n",
    "    asyncio.create_task(model_inference(input_data, model_B_url, False))\n",
    "    logging_mlflow(start_time, \"test\")\n",
    "\n",
    "    logging_mlflow(start_time0, \"total\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [4013]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8002 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     Latency_Base: 0.39160s\n",
      "INFO:     Latency_test: 0.00002s\n",
      "INFO:     Latency_total: 0.41317s\n",
      "INFO:     127.0.0.1:55240 - \"POST /predict HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [4013]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-conda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
